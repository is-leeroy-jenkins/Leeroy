###### Leeroy
![](https://github.com/is-leeroy-jenkins/Leeroy/blob/main/resources/images/leeroy_project.png)

---


A small python application designed for text generation, retrieval-augmented 
generation (RAG), and semantic search  via `llama.cpp`.  The LLM itself is a fine-tuned 
variant of Meta's Llama 3.2 1B Instruct, quantized to Q4_K_M GGUF 
format for high-efficiency, and low-latency inference. 

With strong alignment capabilities, multilingual robustness, and support for complex multi-step reasoning,
Leeroy strikes a balance between performance, size, and instruction quality. Designed for use on CPUs 
and modest GPUs, Leeroy runs natively in llama.cpp, LM Studio, Ollama, and GGUF-compatible environments.


## âœ¨ Key Features

* ğŸ—¨ï¸ **Text Generation** (streaming, llama.cpp native)
* ğŸ“„ **Retrieval Augmentation (Basic RAG)** via document injection
* ğŸ” **Semantic Search** with embeddings (`sentence-transformers`)
* ğŸ§  **Editable System Instructions**
* ğŸ’¾ **Persistent Chat History** (SQLite)
* ğŸ“¤ **Export Conversations** to Markdown and PDF
* ğŸ§® **Live Token Usage Meter**
* âš™ï¸ **Dynamic Parameter Controls** (temperature, top-p, top-k, context, etc.)
* ğŸ–¥ï¸ **Runs Fully Offline** (CPU-only supported)

---
[![Streamlit App](https://img.shields.io/badge/Streamlit-App-FF4B4B?logo=streamlit&logoColor=white)](https://leeroy-py.streamlit.app/)

![](https://github.com/is-leeroy-jenkins/Leeroy/blob/main/resources/images/leeroy-streamlit.gif)

## ğŸ§± Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Leeroy App            â”‚
â”‚        (Streamlit)           â”‚
â”‚                              â”‚
â”‚  - UI / Tabs                 â”‚
â”‚  - RAG / Semantic Search     â”‚
â”‚  - SQLite Persistence        â”‚
â”‚                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Leeroy LLM (GGUF)        â”‚
â”‚  Llama-3.2-1B-Instruct       â”‚
â”‚  Q4_K_M Quantized            â”‚
â”‚                              â”‚
â”‚  Loaded via llama.cpp        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

The **application and the model are intentionally decoupled**:

* The app lives in GitHub
* The model lives on Hugging Face
* Users control where the model is stored locally



## ğŸ“¦ Requirements

### Python

* **Python 3.10+** recommended

### System

* CPU-only supported
* No GPU required
* Windows, macOS, Linux supported


## ğŸ”§ Setting Up the Application

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/<your-org>/leeroy.git
cd leeroy
```

---

### 2ï¸âƒ£ Create a Virtual Environment

#### Windows (PowerShell)

```powershell
python -m venv .venv
.venv\Scripts\activate
```

#### macOS / Linux

```bash
python3 -m venv .venv
source .venv/bin/activate
```

---

### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

---

## ğŸ§  Downloading the Leeroy LLM (Required)

The Leeroy application **does not ship with the model**.

Download the file:

ğŸ‘‰ [![HuggingFace](https://huggingface.co/datasets/huggingface/badges/resolve/main/model-on-hf-sm.svg)](https://huggingface.co/leeroy-jankins/leeroy)



```
Leeroy-3B-Instruct.Q4_K_M.gguf
```

### Recommended Locations

You may place the model **anywhere on your system**, for example:

* LM Studio model cache
* A shared `llm/` directory
* An external drive

Example (Windows):

```
C:\Users\<you>\source\llm\lmstudio\lmstudio-community\
leeroy-jankins\leeroy\Leeroy-3B-Instruct.Q4_K_M.gguf
```

---

## ğŸŒ± Environment Variable Configuration

The application locates the model via an **environment variable**.

### Required Variable

```
LEEROY_LLM_PATH
```

### Windows (PowerShell)

```powershell
setx LEEROY_LLM_PATH "C:\path\to\Leeroy-3B-Instruct.Q4_K_M.gguf"
```

Restart your terminal after setting it.

### macOS / Linux

```bash
export LEEROY_LLM_PATH=/path/to/Leeroy-3B-Instruct.Q4_K_M.gguf
```

---

## â–¶ï¸ Running the Application

```bash
streamlit run app.py
```

Then open your browser at:

```
http://localhost:8501
```

---

## ğŸ§­ Application Tabs

| Tab                        | Description                              |
| -------------------------- | ---------------------------------------- |
| **System Instructions**    | Define global assistant behavior         |
| **Text Generation**        | Chat with Leeroy (streaming)             |
| **Retrieval Augmentation** | Upload documents for basic RAG           |
| **Semantic Search**        | Vector-based retrieval (optional toggle) |
| **Export**                 | Download chat history as Markdown or PDF |

---

## ğŸ§® Token Usage & Parameters

* **Footer (Left)**: prompt tokens, response tokens, % of context used
* **Footer (Right)**: live model parameters
* Updates automatically after each generation

---

## ğŸ“š About the Leeroy LLM

The underlying model is **Leeroy**, a fine-tuned and quantized variant of Metaâ€™s `Llama-3.2-1B-Instruct`, optimized for **local inference and instruction following**.

Key characteristics (from the model README):

* GGUF `Q4_K_M` quantization
* Optimized for llama.cpp, LM Studio, Ollama
* Strong performance on reasoning, policy, and technical tasks
* Trained on curated regulatory and financial datasets
* Apache-2.0 licensed base model with downstream licensing considerations

For full model details, training data, benchmarks, and usage examples, see the Hugging Face README .

---

## ğŸ”’ Notes on Licensing & Usage

* This repository contains **application code only**
* The model is governed by its **own license on Hugging Face**
* Users are responsible for compliance with:

  * Meta Llama license
  * Dataset terms
  * Organizational policies

---

## ğŸ§° Troubleshooting

* **Model not found on startup**
  â†’ Verify `LEEROY_LLM_PATH` and restart your terminal

* **Slow generation**
  â†’ Increase CPU threads, reduce context window

* **High memory usage**
  â†’ Lower context (`n_ctx`) or reduce max tokens

---

## ğŸš€ Roadmap (Application)

* Context window warnings
* Per-turn token deltas
* Conversation management
* Optional model auto-download helper (opt-in)
* Multimodal expansion (when supported by model)

---

## ğŸ Acknowledgements

* **Meta** â€” Llama-3.x base models
* **llama.cpp community**
* **LM Studio**
* **Hugging Face**
* **Sentence-Transformers**

---

If you want, next we can:

* tighten this for a **public OSS audience**
* add screenshots / GIFs
* split â€œModel Setupâ€ into a dedicated doc
* generate a matching `README` for PyPI / Streamlit Cloud

Just say the word.
